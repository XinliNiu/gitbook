# 数学基础



## 信息论

 信息论是通信中的概念，但是其核心概念“熵”在机器学习中也有广泛的应用，例如决策树里的ID3模型和C4.5都是与熵有关的。那么何为熵呢？

熵最开始是物理学里的一个概念，用来描述混乱程度。 一个系统的混乱度越高它的熵就越高，例如把一个排序好的序列打乱，那么它的熵就会变大。香农把熵引入信息论中，用来衡量信息量的大小，叫做信息熵，它可以用来衡量一个信源发出的信号的信息量。如果信号的不确定性越大，那么它的混乱程度也越高，信息熵也高，那么信息量也大。

考虑一个信源发出的信号为X，它的取值为{0,1}，假如我们知道X=1的概率为100%，那么这里的信息熵就为0了，也就是信息量为0，因为确定的事情是没有信息量的。而假如X=1的概率为50%，X=0的概率也为50%，那么它的不确定性就很大，信息熵很高，信息量也很大。那具体怎么衡量呢？

假设随机变量X的取值范围是 $$\{x_1,x_2,...,x_n\}$$ ，概率分布为 $$P(X=x_i)=p_i(i\in\{1,2,...,n\})$$ ，那么随机变量X的信息熵为:

$$
H(X)=-\sum_{i}^{n}(P(x_i)log(P(x_i))=\sum_{i}^{n}\frac{P(x_i)}{log(P(x_i))}
$$

假设X的取值为{0,1}，而P\(X=0\)=p的话，那么P\(X=1\)=1-p，此时X的熵为

$$
H(X)=-plog\cdot{(p)}  - (1-p)\cdot{log}(1-p) , 0 \le{p}\le{1}
$$

如果我们把这个函数画出来，它的形状如下：

（图）

 由图可见，二元信源的信息熵具有：

* 非负性，即收到一个信源符号所获得的信息量应为正值，H（X）≥0；
* 对称性，即对称于P=0．5；
* 确定性，H（1，0）=0，即P=0或P=1已是确定状态，所得信息量为零；
* 极值性，当P=0．5时，H（X）最大；而且H（X）是P的上凸函数。

同样，对于n源信源，也具有类似的性质。








# 梯度下降与鞍点

今天在用Tensorflow Playground演示神经网络，发现了一个问题，对于下面的分类问题，使用最简单的能表达这个分类的\(2,2,1\)网络，大部分情况下都训练不出来，大约10次里有一次能找到分类边界。

作为一个

这个Tensorflow Playground用js实现了一个简单的backpropagation算法，里面用的代价函数是平方差，也就是

$$
Error=\frac{1}{2m}\sum_{i=1}^{m}(\hat y^i-y^i)^2
$$

其中m是训练集的个数， $$\hat y^i$$ 是第i个数据第预测值， $$y^i$$ 是第i个数据第真实值。

均方差一般用于回归问题，而分类问题不用，例如逻辑回归和神经网络。为什么不用呢？用它衡量一个模型第好坏是可以的，但是用它求解最优参数不合适，因为它不是凸函数，有很多局部极小值，而梯度下降一旦进入局部极小值，就出不来了。



在机器学习中，要优化的函数第变量有几百上千甚至上百万个，我们没法把函数第图做出来，甚至都没法想象。我们只能凭想象与一些数学推导来理解为什么均方差不适合用于逻辑回归和神经网络。

以最简单的逻辑回归为例，特征值有两个，激活函数为sigmoid函数。模型如下

$$
\hat y^i=sigmoid(w_1x^i_1+w_2x^i_2+b)
$$

$$x^i$$ 是一个输入向量，表示第i个训练数据，每个输入有 $$x_1,x_2$$ ，这个模型有三个参数 $$w_1,w_2,b$$ 。其中sigmoid函数定义如下，我们 $$\sigma$$ 表示。

$$
\sigma(x)=\frac{1}{1+e^{-x}}
$$

sigmoid函数有一个性质，很容易证，就是

$$
\sigma'(x)=\sigma(x)(1-\sigma(x))
$$

使用MSE作为逻辑回归的loss function。

$$
J = \frac{1}{2}\sum_{i=1}^{n}(\sigma(w_1x_1^{i}+w_2x_2^{i}+b)-y^i)^2
$$

为了方便，其中的 $$\sigma$$ 就简写为 $$\sigma$$ ，我们对三个参数分别求导

$$
\frac{\partial J}{\partial w_1}=\sum_{i=1}^{n}(\sigma-y^i)\sigma(1-\sigma)x^i_1 \\
\frac{\partial J}{\partial w_2}=\sum_{i=1}^{n}(\sigma-y^i)\sigma(1-\sigma)x^i_2 \\
\frac{\partial J}{\partial b}=\sum_{i=1}^{n}(\sigma-y^i)\sigma(1-\sigma)
$$

在梯度下降的过程中，如果找到了一组参数，能够让上面三个导数都非常接近0，那么我们就认为找到最优值了。但是导数为0，并不代表代价函数是最小的。假如我们找到了一组参数，把所有的预测值都预测成了1\(实际上用于小于1\)，那么把预测值带进去，会发现上面的三个导数都接近0，但是实际上代价函数却很大。也就是只要我们的预测值接近1或者0，而不是0.5，0.7这样的，那么导数都接近0。



w1=20,w2=0,b=0

| $$x_1$$  | $$x_2$$  | $$y$$  | $$\hat y$$ \( $$\sigma$$ \) |
| :--- | :--- | :--- | :--- |
| -1 | 1 | 1 | 约为0 |
| 1 | 1 | 1 | 约为1 |
| -1 | -1 | 0 | 约为0 |
| 1 | -1 | 0 | 约为1 |













$$
J=\frac{1}{m}\sum_{i=1}^{m}(y^ilog(\hat y^i)+(1-y^i)log(1-\hat y^i))
$$

$$
\frac{\partial J}{\partial w_1}=\frac{1}{m}\sum_{i=1}^{m}(y^i-\sigma)x^i_1 \\
\frac{\partial J}{\partial w_2}=\frac{1}{m}\sum_{i=1}^{m}(y^i-\sigma)x^i_2 \\
\frac{\partial J}{\partial b}=\frac{1}{m}\sum_{i=1}^{m}(y^i-\sigma)
$$

我们可以看到，要想让这三个导数都为0，那么必须是预测值接近真实值才行。也就是如果我们找到了一组参数，让

